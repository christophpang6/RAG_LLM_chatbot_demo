{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This has been tested in colab environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdJ57I0QrCtW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"enter-api-key-here\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoPeym9nq_os"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ======== Install dependencies ========\n",
        "!pip install -q sentence-transformers faiss-cpu torch datasets evaluate rouge-score openai gradio\n",
        "\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "\n",
        "print(\"=== Step 1: Dependencies imported ===\")\n",
        "\n",
        "# ======== OpenAI Setup ========\n",
        "openai_api_key = os.environ.get(\"OPENAI_API_KEY\", \"your-api-key-here\")\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "print(\"=== Step 2: OpenAI client configured ===\")\n",
        "\n",
        "# ======== Sample Training Documents ========\n",
        "print(\"=== Step 3: Loading training documents ===\")\n",
        "enhanced_sample_texts = {\n",
        "    \"space_missions.txt\": \"\"\"\n",
        "    The Apollo 11 mission launched on July 16, 1969, and landed the first humans on the Moon on July 20, 1969.\n",
        "    The crew consisted of exactly three astronauts: Neil Armstrong (Commander), Buzz Aldrin (Lunar Module Pilot),\n",
        "    and Michael Collins (Command Module Pilot). Neil Armstrong was the first person to walk on the Moon,\n",
        "    followed by Buzz Aldrin. Michael Collins remained in lunar orbit aboard the command module Columbia.\n",
        "    The mission lasted 8 days, 3 hours, 18 minutes, and 35 seconds. There was no fourth crew member on Apollo 11.\n",
        "    \"\"\",\n",
        "    \"landmarks_architecture.txt\": \"\"\"\n",
        "    The Eiffel Tower is a wrought-iron lattice tower located on the Champ de Mars in Paris, France.\n",
        "    Construction began in 1887 and was completed in 1889 for the 1889 World's Fair.\n",
        "    \"\"\",\n",
        "    \"programming_technologies.txt\": \"\"\"\n",
        "    Python was created by Guido van Rossum and first released in 1991.\n",
        "    It emphasizes code readability with its notable use of significant whitespace.\n",
        "    \"\"\",\n",
        "    \"science_discoveries.txt\": \"\"\"\n",
        "    Penicillin was discovered by Alexander Fleming in 1928 when he noticed that a mold had killed bacteria in his lab.\n",
        "    \"\"\",\n",
        "    \"historical_events.txt\": \"\"\"\n",
        "    World War II lasted from 1939 to 1945 and involved most of the world's nations.\n",
        "    The war ended with the surrender of Germany on May 8, 1945 (Victory in Europe Day)\n",
        "    and Japan on August 15, 1945, following the atomic bombings of Hiroshima and Nagasaki.\n",
        "    \"\"\"\n",
        "}\n",
        "print(f\"=== Step 3 Complete: Loaded {len(enhanced_sample_texts)} documents ===\")\n",
        "\n",
        "# ======== Prepare Corpus and FAISS Index ========\n",
        "print(\"=== Step 4: Loading SentenceTransformer embedding model ===\")\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "print(\"Embedding model loaded.\")\n",
        "\n",
        "corpus, sources = [], []\n",
        "print(\"=== Step 5: Preparing corpus for FAISS index ===\")\n",
        "for src, text in enhanced_sample_texts.items():\n",
        "    for line in text.strip().split(\"\\n\"):\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            corpus.append(line)\n",
        "            sources.append(src)\n",
        "print(f\"Corpus prepared with {len(corpus)} lines.\")\n",
        "\n",
        "print(\"=== Step 6: Generating embeddings for corpus ===\")\n",
        "embeddings = embedder.encode(corpus, convert_to_numpy=True)\n",
        "print(\"Embeddings generated.\")\n",
        "\n",
        "print(\"=== Step 7: Creating FAISS index ===\")\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "print(\"FAISS index created and embeddings added.\")\n",
        "\n",
        "# ======== RAG Retrieval + OpenAI Chat Function ========\n",
        "VERBOSE = True\n",
        "chat_history = []\n",
        "\n",
        "def chat_fn(user_query, chat_history_state=None\n",
        "            ):\n",
        "    global chat_history\n",
        "\n",
        "\n",
        "    # Combine previous queries/answers into context\n",
        "    history_text = \"\"\n",
        "    for q, a in chat_history[-3:]:\n",
        "        history_text += f\"Previous Q: {q}\\nPrevious A: {a}\\n\"\n",
        "\n",
        "    # FAISS retrieval\n",
        "    # Create retrieval query using last 3 turns + current query\n",
        "    retrieval_query = \" \".join([f\"{q} {a}\" for q, a in chat_history[-3:]] + [user_query])\n",
        "    q_emb = embedder.encode([retrieval_query], convert_to_numpy=True)\n",
        "    D, I = index.search(q_emb, k=5)\n",
        "    retrieved_chunks = [(corpus[i], sources[i], D[0][j]) for j, i in enumerate(I[0])]\n",
        "    context_text = \"\\n\".join([f\"[{src}] {chunk}\" for chunk, src, _ in retrieved_chunks])\n",
        "\n",
        "\n",
        "\n",
        "    if VERBOSE:\n",
        "        print(f\"\\n=== Processing Query: {user_query} ===\")\n",
        "        print(\"Retrieved Chunks (with L2 distance scores):\")\n",
        "        for chunk, src, score in retrieved_chunks:\n",
        "            print(f\"[{src}] Score: {score:.4f} | {chunk}\")\n",
        "        import sys; sys.stdout.flush()\n",
        "\n",
        "    # LLM prompt\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Only answer based on the provided context. If the context does not contain the answer, say 'I don't know.'\"},\n",
        "        {\"role\": \"user\", \"content\": f\"{history_text}\\nCurrent Query: {user_query}\\nContext:\\n{context_text}\"}\n",
        "    ]\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "        max_tokens=300\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Update global chat history\n",
        "    chat_history.append((user_query, answer))\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n",
        "# ======== Launch Gradio Interface ========\n",
        "print(\"=== Step 8: Creating Gradio ChatInterface ===\")\n",
        "model_name = \"gpt-4\"\n",
        "chat_interface = gr.ChatInterface(\n",
        "    fn=chat_fn,\n",
        "    title=f\"Multi-Turn RAG Demo ({model_name})\",\n",
        "    description=\"Ask questions about space missions, landmarks, programming, science, or historical events. The system will show retrieved chunks and provide answers.\",\n",
        "    examples=[\n",
        "        \"Who were the astronauts on Apollo 11?\",\n",
        "        \"When was the Eiffel Tower built?\",\n",
        "        \"Who created Python?\",\n",
        "        \"When did World War II end?\"\n",
        "    ],\n",
        "    theme=\"soft\",\n",
        "    css=\"\"\"\n",
        "    .gradio-chatbox {\n",
        "        min-height: 500px;  /* increase height as desired */\n",
        "    }\n",
        "  \"\"\"\n",
        "\n",
        ")\n",
        "print(\"âœ“ Gradio interface created\")\n",
        "\n",
        "print(\"\\n=== Step 9: Launching Gradio app ===\")\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ðŸš€ Starting Gradio server...\")\n",
        "    chat_interface.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
